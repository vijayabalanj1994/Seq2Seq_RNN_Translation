{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8402e76f-3ea6-4023-adc1-e743ed946964",
   "metadata": {},
   "source": [
    "## Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0333f49c-a1e5-491a-8195-e4319992527d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppressing warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2168c0b4-3e42-4f8d-9349-1f729a61b474",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---- ----------------------------------- 1.3/12.8 MB 13.4 MB/s eta 0:00:01\n",
      "     --------- ------------------------------ 2.9/12.8 MB 7.3 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 4.7/12.8 MB 7.7 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 6.8/12.8 MB 8.4 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 9.2 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.8/12.8 MB 9.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 9.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Collecting de-core-news-sm==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl (14.6 MB)\n",
      "     ---------------------------------------- 0.0/14.6 MB ? eta -:--:--\n",
      "     ----- ---------------------------------- 2.1/14.6 MB 13.0 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 4.7/14.6 MB 12.4 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 7.3/14.6 MB 12.6 MB/s eta 0:00:01\n",
      "     -------------------------- ------------ 10.0/14.6 MB 12.4 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 12.6/14.6 MB 12.3 MB/s eta 0:00:01\n",
      "     --------------------------------------- 14.6/14.6 MB 12.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from de-core-news-sm==3.7.0) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.9.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2024.8.30)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.1.3)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Iterable, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "\n",
    "from torchdata.datapipes.iter import IterableWrapper, Mapper\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee1b9c6-2ce8-414d-9605-a95fb1668d94",
   "metadata": {},
   "source": [
    "#### Checking if CUDA is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e408c82-34bf-4bd3-8989-af6b3a5d49b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c5d3c8-859b-4db0-9dc9-2a558f44623f",
   "metadata": {},
   "source": [
    "## The Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34e75790-ea82-443b-a316-590774d8910e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a rrn implemented using \"nn.EmbeddingBag\", \"nn.LSTM\", \"nn.Dropout\" functions from \"pytorch\" library\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_len, emb_dim, hid_dim, n_layers, dropout_prob):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_len, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout_prob)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        embed = self.dropout(self.embedding(input_batch))\n",
    "        embed = embed.to(device)\n",
    "        outputs, (hidden, cell) = self.lstm(embed)\n",
    "\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d77f7dd-83cb-40c7-b7bc-3cbf7e777d08",
   "metadata": {},
   "source": [
    "#### Encoder -> example of one forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "237e17fe-82c7-419f-a620-85dd96482fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy data\n",
    "vocab_len = 8\n",
    "emb_dim = 10\n",
    "hid_dim = 8\n",
    "n_layers = 1\n",
    "dropout_prob = 0.5\n",
    "\n",
    "# instantiating the model\n",
    "encoder_t = Encoder(vocab_len, emb_dim, hid_dim, n_layers, dropout_prob).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab5ed3a5-cbf1-4317-8de9-b7ae86fb4f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input(src) tensor  [shape -> 5]:-\n",
      " tensor([0, 3, 4, 2, 1], device='cuda:0')\n",
      "\n",
      "Embedded tokens  [shape -> 5]:-\n",
      " tensor([[ 0.6591, -1.1215,  2.0592, -0.5522,  0.9117, -0.4574,  2.6500, -1.0586,\n",
      "         -0.4957,  1.5255],\n",
      "        [-0.2039,  0.0906,  1.2810, -0.9542, -1.7231,  0.6712, -1.2482, -0.1359,\n",
      "         -0.0165,  0.9027],\n",
      "        [-0.0992,  0.7913, -1.1062, -0.7618, -0.7259, -1.2637,  0.3867,  1.5638,\n",
      "         -0.6415, -1.0602],\n",
      "        [-1.4792, -1.7376, -0.9612,  0.5893,  0.9242, -0.7253, -0.2297,  1.5039,\n",
      "         -1.3460,  0.2714],\n",
      "        [ 1.5120,  1.7224, -0.0128, -0.1236,  0.4383, -0.4804,  0.0082,  1.7586,\n",
      "         -3.0753,  0.4310]], device='cuda:0', grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "After dropout:-  [shape -> 5]\n",
      " tensor([[ 1.3181, -2.2431,  0.0000, -0.0000,  1.8234, -0.0000,  5.3000, -2.1173,\n",
      "         -0.9914,  0.0000],\n",
      "        [-0.0000,  0.0000,  2.5620, -1.9084, -3.4461,  0.0000, -2.4964, -0.2719,\n",
      "         -0.0329,  1.8055],\n",
      "        [-0.1983,  0.0000, -2.2123, -1.5236, -0.0000, -0.0000,  0.0000,  0.0000,\n",
      "         -0.0000, -0.0000],\n",
      "        [-2.9584, -0.0000, -0.0000,  1.1785,  0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         -0.0000,  0.5428],\n",
      "        [ 3.0240,  3.4448, -0.0257, -0.0000,  0.0000, -0.9607,  0.0164,  0.0000,\n",
      "         -6.1505,  0.8620]], device='cuda:0', grad_fn=<NativeDropoutBackward0>)\n",
      "\n",
      "Hidden:-         [shape -> 1]\n",
      " tensor([[ 0.0812, -0.1343, -0.0026,  0.2119, -0.0431, -0.0187,  0.0397, -0.1790]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "\n",
      "Cell:-           [shape -> 1]\n",
      " tensor([[ 0.1363, -0.2816, -0.0176,  0.2743, -0.2190, -0.1073,  0.2279, -0.3841]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "\n",
      "Output:-         [shape -> 5]\n",
      " tensor([[-0.0239, -0.1849,  0.0070, -0.4476,  0.0415,  0.0427, -0.0049, -0.1689],\n",
      "        [ 0.0741,  0.0078, -0.0874, -0.2481, -0.2645,  0.0523,  0.0800, -0.2185],\n",
      "        [-0.1793, -0.1570,  0.1412, -0.1565, -0.0237,  0.1560,  0.0655,  0.0381],\n",
      "        [-0.2827, -0.2165,  0.1410,  0.1434, -0.2609, -0.2568, -0.2582,  0.0332],\n",
      "        [ 0.0812, -0.1343, -0.0026,  0.2119, -0.0431, -0.0187,  0.0397, -0.1790]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# dummy data \n",
    "src_batch = torch.tensor([0,3,4,2,1]) #where 0,3,4,2,1 are vocab indecies\n",
    "src_batch = src_batch.t().to(device)\n",
    "\n",
    "# getting the embedding of the text token indices\n",
    "embedded = encoder_t.embedding(src_batch)\n",
    "# appling dropout to embedded\n",
    "embedded_dropout = encoder_t.dropout(embedded)\n",
    "embedded_dropout.to(device)\n",
    "# passing through the lstm\n",
    "outputs, (hidden_t, cell_t) = encoder_t.lstm(embedded_dropout)\n",
    "\n",
    "print(f\"Input(src) tensor  [shape -> {src_batch.shape[0]}]:-\\n\", src_batch)\n",
    "print(f\"\\nEmbedded tokens  [shape -> {embedded.shape[0]}]:-\\n\", embedded)\n",
    "print(f\"\\nAfter dropout:-  [shape -> {embedded_dropout.shape[0]}]\\n\", embedded_dropout)\n",
    "print(f\"\\nHidden:-         [shape -> {hidden_t.shape[0]}]\\n\", hidden_t)\n",
    "print(f\"\\nCell:-           [shape -> {cell_t.shape[0]}]\\n\", cell_t)\n",
    "print(f\"\\nOutput:-         [shape -> {outputs.shape[0]}]\\n\", outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee057f9-8b8f-45f1-a23a-564783ef295b",
   "metadata": {},
   "source": [
    "## The Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6c2002d-fc3c-41b3-859a-269301a5c773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a rrn implemented using \"nn.EmbeddingBag\", \"nn.Linear\" ,\"nn.LSTM\", \"nn.Dropout\", \"nn.LogSoftmax\" functions from \"pytorch\" library\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        # input = [batch_size]\n",
    "        input = input.unsqueeze(0) # input = [1, batch_size]\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        prediction_logit = self.fc_out(output.squeeze(0))\n",
    "        prediction = self.softmax(prediction_logit)\n",
    "\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570ffdac-576e-4889-8f6e-1b366a78e748",
   "metadata": {},
   "source": [
    "#### Decoder -> example of one forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24f2a99d-786d-4e4e-ba28-1b1b80d02a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy data\n",
    "output_dim = 6\n",
    "emb_dim = 10\n",
    "hid_dim = 8\n",
    "n_layers = 1\n",
    "dropout = 0.5\n",
    "\n",
    "# instantiating the model\n",
    "decoder_t = Decoder(output_dim, emb_dim, hid_dim, n_layers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b8eccd1-f45e-4d29-bb7c-04ced18b3481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input(target) tensor  [shape -> 1]:-\n",
      " tensor([0], device='cuda:0')\n",
      "\n",
      "Embedded tokens  [shape -> 1]:-\n",
      " tensor([[-1.8793,  0.2864,  0.0866,  0.3047, -0.1428, -0.4833, -2.7147, -0.0226,\n",
      "         -2.0847, -0.2899]], device='cuda:0', grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "After dropout:-  [shape -> 1]\n",
      " tensor([[-0.0000,  0.0000,  0.1731,  0.0000, -0.2856, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000]], device='cuda:0', grad_fn=<NativeDropoutBackward0>)\n",
      "\n",
      "RNN Output:-     [shape -> 1]\n",
      " tensor([[ 0.0945,  0.0011,  0.0397, -0.0620,  0.1445, -0.1348,  0.0785, -0.2011]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "\n",
      "FC layer Out:-   [shape -> 1]\n",
      " tensor([[ 0.3602, -0.0872, -0.0362, -0.2333,  0.2648, -0.2661]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Softmax to Out:- [shape -> 1]\n",
      " tensor([[-1.4605, -1.9078, -1.8568, -2.0540, -1.5559, -2.0868]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# dummy data \n",
    "input_t = torch.tensor([0]).to(device) # <bos>\n",
    "\n",
    "# getting the embedding of the input text token <bos>\n",
    "embedded = decoder_t.embedding(input_t)\n",
    "# appling dropout to embedded\n",
    "embedded_dropout = decoder_t.dropout(embedded)\n",
    "embedded_dropout.to(device)\n",
    "# passing through the lstm\n",
    "output, (hidden, cell) = decoder_t.lstm(embedded_dropout, (hidden_t, cell_t))\n",
    "# passing through the fully connected layer\n",
    "prediction_logit = decoder_t.fc_out(output)\n",
    "# appling softmax\n",
    "prediction = decoder_t.softmax(prediction_logit)\n",
    "\n",
    "print(f\"Input(target) tensor  [shape -> {input_t.shape[0]}]:-\\n\", input_t)\n",
    "print(f\"\\nEmbedded tokens  [shape -> {embedded.shape[0]}]:-\\n\", embedded)\n",
    "print(f\"\\nAfter dropout:-  [shape -> {embedded_dropout.shape[0]}]\\n\", embedded_dropout)\n",
    "#print(f\"\\nRNN Hidden:-     [shape -> {hidden.shape[0]}]\\n\", hidden)\n",
    "#print(f\"\\nRNN Cell:-       [shape -> {cell.shape[0]}]\\n\", cell)\n",
    "print(f\"\\nRNN Output:-     [shape -> {output.shape[0]}]\\n\", output)\n",
    "print(f\"\\nFC layer Out:-   [shape -> {prediction_logit.shape[0]}]\\n\", prediction_logit)\n",
    "print(f\"\\nSoftmax to Out:- [shape -> {prediction.shape[0]}]\\n\", prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db45b4d-453a-4e4f-95c7-b7cb5abb6707",
   "metadata": {},
   "source": [
    "#### Encoder-decoder connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37e541ec-6b69-45ac-ba22-9b49840d3872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output in each time step [shape -> torch.Size([5, 1, 6])]:-\n",
      " tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-1.7104, -1.5063, -2.1073, -2.0045, -1.8040, -1.7341]],\n",
      "\n",
      "        [[-1.6352, -1.6171, -2.0413, -2.0507, -1.7456, -1.7512]],\n",
      "\n",
      "        [[-1.8648, -1.5223, -2.0661, -2.0419, -1.6634, -1.7097]],\n",
      "\n",
      "        [[-1.9186, -1.4293, -2.1572, -2.0169, -1.6606, -1.7432]]],\n",
      "       device='cuda:0', grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "# dummy data \n",
    "src = torch.tensor([[0,3,4,2,1]]) #where 0,3,4,2,1 are vocab indecies\n",
    "src = src.t().to(device)\n",
    "# instantiating the encoder model\n",
    "encoder_t = Encoder(vocab_len, emb_dim, hid_dim, n_layers, dropout_prob).to(device)\n",
    "# geting the encoder output\n",
    "hidden_t , cell_t = encoder_t(src)\n",
    "\n",
    "\n",
    "\n",
    "# dummy data \n",
    "trg = torch.tensor([[0],[2],[3],[5],[1]]).to(device) # 0 -> <bos>\n",
    "# instantiating the decoder model\n",
    "decoder_t = Decoder(output_dim, emb_dim, hid_dim, n_layers, dropout).to(device)\n",
    "\n",
    "# a tensor to store decoder opuputs at each time step\n",
    "batch_size = trg.shape[1]\n",
    "trg_len = trg.shape[0]\n",
    "trg_vocab_size = decoder_t.output_dim\n",
    "outputs_t = torch.zeros(trg_len, batch_size, trg_vocab_size).to(device)\n",
    "\n",
    "# the first input to the decoder is the <bos> token\n",
    "input = trg[0,:]\n",
    "\n",
    "# looping through the trg length\n",
    "for t in range(1, trg_len):\n",
    "    output_t, hidden_t, cell_t = decoder_t(input, hidden_t, cell_t)\n",
    "    # storing the output in current time step\n",
    "    outputs_t[t] = output_t\n",
    "    #getting the predicted token  index\n",
    "    top_1 = output_t.argmax(1)\n",
    "    # deciding weather to use tracher forcing\n",
    "    teacher_force = random.random() < 0.5 # 0.5 -> teacher forcing ratio\n",
    "    input = trg[t] if teacher_force else top_1\n",
    "\n",
    "print(f\"Decoder output in each time step [shape -> {outputs_t.shape}]:-\\n\",outputs_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91a01920-d114-4eda-bc56-671b35b11a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# making predictions\n",
    "pred_tokens = outputs_t.argmax(2)\n",
    "print(pred_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0baee6-8f84-4a61-a2e0-8927e3b16d27",
   "metadata": {},
   "source": [
    "## Sequence-to-sequence model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "118bbb81-ff0f-4efd-b989-3fb6e07f7f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting encoder and decoder components to create the seq2seq model\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device, trg_vocab):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        self.trg_vocab = trg_vocab\n",
    "\n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimension of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(device)\n",
    "\n",
    "        #last encoder hidden state\n",
    "        hidden, cell = self.encode(src)\n",
    "        hidden.to(device)\n",
    "        cell.to(device)\n",
    "\n",
    "        #first input to decoder is <bos>\n",
    "        input = trg[0,:]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            output , hidden, cell = self.decoder(input, hidden, cell)\n",
    "             # storing the output from the current time step\n",
    "            outputs[t] = output\n",
    "            #getting the predicted token index\n",
    "            top_1 = output.argmax(1)\n",
    "            # deciding weather to use tracher forcing\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            input = trg[t] if  teacher_force else top_1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9c7158-252c-434e-b754-a0ff8a46de79",
   "metadata": {},
   "source": [
    "#### A function to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b59d9fd-27e0-48cb-ba52-1c3cc66414ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    train_iterator = tqdm(iterator, desc=\"Training\", leavel=False)\n",
    "\n",
    "    for i, (src, trg) in enumerate(iterator):\n",
    "        # senting the src anf trg tensors to device\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "\n",
    "        # clearing the gradinet from previous batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # the models predictions - token probabilities\n",
    "        output = model(src, trg)\n",
    "\n",
    "        #----- trg shape -> [trg len, batch_size]\n",
    "        #----- output shape -> [trg_len, batch_size, output_dim]\n",
    "\n",
    "        output_dim = output.size[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].contiguous().view(-1)\n",
    "\n",
    "        #----- trg shape -> [(trg len -1) * batch_size]\n",
    "        #----- output shape -> [(trg len -1) * batch_size, output dim]\n",
    "\n",
    "        # computing the loss\n",
    "        loss = creterion(output, trg)\n",
    "        # compiting the gradient\n",
    "        loss.backward()\n",
    "        # cliping the gradien\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        # updating the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Updating the tqdm\n",
    "        train_iterator.set_postfix(loss=loss.)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
