{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8402e76f-3ea6-4023-adc1-e743ed946964",
   "metadata": {},
   "source": [
    "## Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0333f49c-a1e5-491a-8195-e4319992527d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppressing warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2168c0b4-3e42-4f8d-9349-1f729a61b474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.5/12.8 MB 8.5 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 3.4/12.8 MB 10.6 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 17.8 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 18.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Collecting de-core-news-sm==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl (14.6 MB)\n",
      "     ---------------------------------------- 0.0/14.6 MB ? eta -:--:--\n",
      "     ------ --------------------------------- 2.4/14.6 MB 19.2 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 8.7/14.6 MB 25.6 MB/s eta 0:00:01\n",
      "     --------------------------------------- 14.6/14.6 MB 27.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from de-core-news-sm==3.7.0) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.9.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2024.8.30)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vijay\\anaconda3\\envs\\ibm_cuda\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.1.3)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Iterable, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.functional as F\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "\n",
    "from torchdata.datapipes.iter import IterableWrapper, Mapper\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee1b9c6-2ce8-414d-9605-a95fb1668d94",
   "metadata": {},
   "source": [
    "#### Checking if CUDA is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e408c82-34bf-4bd3-8989-af6b3a5d49b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c5d3c8-859b-4db0-9dc9-2a558f44623f",
   "metadata": {},
   "source": [
    "## The Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34e75790-ea82-443b-a316-590774d8910e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a rrn implemented using \"nn.EmbeddingBag\", \"nn.LSTM\", \"nn.Dropout\" functions from \"pytorch\" library\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_len, emb_dim, hid_dim, n_layers, dropout_prob):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_len, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout_prob)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        embed = self.dropout(self.embedding(input_batch))\n",
    "        embed = embed.to(device)\n",
    "        outputs, (hidden, cell) = self.lstm(embed)\n",
    "\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d77f7dd-83cb-40c7-b7bc-3cbf7e777d08",
   "metadata": {},
   "source": [
    "#### Encoder -> example of one forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "237e17fe-82c7-419f-a620-85dd96482fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy data\n",
    "vocab_len = 8\n",
    "emb_dim = 10\n",
    "hid_dim = 8\n",
    "n_layers = 1\n",
    "dropout_prob = 0.5\n",
    "\n",
    "# instantiating the model\n",
    "encoder_t = Encoder(vocab_len, emb_dim, hid_dim, n_layers, dropout_prob).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab5ed3a5-cbf1-4317-8de9-b7ae86fb4f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input(src) tensor  [shape -> 5]:-\n",
      " tensor([0, 3, 4, 2, 1], device='cuda:0')\n",
      "\n",
      "Embedded tokens  [shape -> 5]:-\n",
      " tensor([[-0.4376, -0.6285,  0.5262, -1.5030,  0.4078,  0.1611, -0.7881, -1.1664,\n",
      "          0.1138,  1.7444],\n",
      "        [-2.4943,  2.0648, -1.1426, -0.0342, -0.0481,  1.6496,  0.3613,  0.3524,\n",
      "         -0.0836,  0.3201],\n",
      "        [-1.2027,  0.2486, -0.1388, -1.3380,  0.4217,  2.6053,  0.8107, -0.9168,\n",
      "          0.4913, -0.9330],\n",
      "        [ 0.6603, -0.4444, -1.8094,  0.2662, -1.8565,  0.4387,  0.5044,  0.4950,\n",
      "          0.1709, -0.8362],\n",
      "        [ 0.0616, -1.4352, -0.7731, -0.8827, -0.3496, -1.1252,  0.1401, -0.7673,\n",
      "          1.3692,  2.6370]], device='cuda:0', grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "After dropout:-  [shape -> 5]\n",
      " tensor([[-0.8753, -1.2570,  0.0000, -0.0000,  0.0000,  0.3223, -0.0000, -0.0000,\n",
      "          0.0000,  3.4889],\n",
      "        [-0.0000,  4.1297, -0.0000, -0.0683, -0.0961,  0.0000,  0.7226,  0.0000,\n",
      "         -0.0000,  0.0000],\n",
      "        [-0.0000,  0.4972, -0.2777, -0.0000,  0.8435,  0.0000,  1.6214, -0.0000,\n",
      "          0.9827, -1.8660],\n",
      "        [ 1.3205, -0.8889, -3.6187,  0.5323, -0.0000,  0.8775,  1.0088,  0.0000,\n",
      "          0.0000, -0.0000],\n",
      "        [ 0.0000, -2.8704, -1.5463, -1.7655, -0.6991, -0.0000,  0.0000, -1.5346,\n",
      "          2.7384,  5.2740]], device='cuda:0', grad_fn=<NativeDropoutBackward0>)\n",
      "\n",
      "Hidden:-         [shape -> 1]\n",
      " tensor([[-0.0738,  0.2472,  0.2186,  0.3579, -0.1347, -0.0835, -0.1416,  0.0385]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "\n",
      "Cell:-           [shape -> 1]\n",
      " tensor([[-0.9343,  0.5228,  0.3244,  0.5961, -0.2437, -0.5906, -0.6558,  0.0451]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "\n",
      "Output:-         [shape -> 5]\n",
      " tensor([[ 0.0245,  0.2396,  0.1838,  0.2494, -0.0893,  0.0763, -0.0244,  0.2039],\n",
      "        [ 0.2371, -0.0039, -0.1902, -0.3250, -0.0560,  0.1028,  0.1978, -0.0997],\n",
      "        [-0.0539, -0.1267, -0.3425, -0.0494, -0.2165, -0.0195,  0.0108, -0.1890],\n",
      "        [-0.1060, -0.0584, -0.1735, -0.0196, -0.1353, -0.0621, -0.0405, -0.0447],\n",
      "        [-0.0738,  0.2472,  0.2186,  0.3579, -0.1347, -0.0835, -0.1416,  0.0385]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# dummy data \n",
    "src_batch = torch.tensor([0,3,4,2,1]) #where 0,3,4,2,1 are vocab indecies\n",
    "src_batch = src_batch.t().to(device)\n",
    "\n",
    "# getting the embedding of the text token indices\n",
    "embedded = encoder_t.embedding(src_batch)\n",
    "# appling dropout to embedded\n",
    "embedded_dropout = encoder_t.dropout(embedded)\n",
    "embedded_dropout.to(device)\n",
    "# passing through the lstm\n",
    "outputs, (hidden_t, cell_t) = encoder_t.lstm(embedded_dropout)\n",
    "\n",
    "print(f\"Input(src) tensor  [shape -> {src_batch.shape[0]}]:-\\n\", src_batch)\n",
    "print(f\"\\nEmbedded tokens  [shape -> {embedded.shape[0]}]:-\\n\", embedded)\n",
    "print(f\"\\nAfter dropout:-  [shape -> {embedded_dropout.shape[0]}]\\n\", embedded_dropout)\n",
    "print(f\"\\nHidden:-         [shape -> {hidden_t.shape[0]}]\\n\", hidden_t)\n",
    "print(f\"\\nCell:-           [shape -> {cell_t.shape[0]}]\\n\", cell_t)\n",
    "print(f\"\\nOutput:-         [shape -> {outputs.shape[0]}]\\n\", outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee057f9-8b8f-45f1-a23a-564783ef295b",
   "metadata": {},
   "source": [
    "## The Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6c2002d-fc3c-41b3-859a-269301a5c773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a rrn implemented using \"nn.EmbeddingBag\", \"nn.Linear\" ,\"nn.LSTM\", \"nn.Dropout\", \"nn.LogSoftmax\" functions from \"pytorch\" library\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        # input = [batch_size]\n",
    "        input = input.unsqueeze(0) # input = [1, batch_size]\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        prediction_logit = self.fc_out(output.squeeze(0))\n",
    "        prediction = self.softmax(prediction_logit)\n",
    "\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570ffdac-576e-4889-8f6e-1b366a78e748",
   "metadata": {},
   "source": [
    "#### Decoder -> example of one forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24f2a99d-786d-4e4e-ba28-1b1b80d02a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy data\n",
    "output_dim = 6\n",
    "emb_dim = 10\n",
    "hid_dim = 8\n",
    "n_layers = 1\n",
    "dropout = 0.5\n",
    "\n",
    "# instantiating the model\n",
    "decoder_t = Decoder(output_dim, emb_dim, hid_dim, n_layers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b8eccd1-f45e-4d29-bb7c-04ced18b3481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input(target) tensor  [shape -> 1]:-\n",
      " tensor([0], device='cuda:0')\n",
      "\n",
      "Embedded tokens  [shape -> 1]:-\n",
      " tensor([[-0.4480,  0.5212,  0.3156,  1.4902,  0.2587,  1.2582,  1.4674,  0.7292,\n",
      "         -0.7046,  0.3079]], device='cuda:0', grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "After dropout:-  [shape -> 1]\n",
      " tensor([[-0.8960,  1.0425,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.4584,\n",
      "         -1.4093,  0.0000]], device='cuda:0', grad_fn=<NativeDropoutBackward0>)\n",
      "\n",
      "RNN Output:-     [shape -> 1]\n",
      " tensor([[-0.2753, -0.0379,  0.1973,  0.1429, -0.1729, -0.1386, -0.1699, -0.1995]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "\n",
      "FC layer Out:-   [shape -> 1]\n",
      " tensor([[-0.3074, -0.0430,  0.1013, -0.1009,  0.0900,  0.1081]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Softmax to Out:- [shape -> 1]\n",
      " tensor([[-2.0845, -1.8200, -1.6757, -1.8779, -1.6870, -1.6689]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# dummy data \n",
    "input_t = torch.tensor([0]).to(device) # <bos>\n",
    "\n",
    "# getting the embedding of the input text token <bos>\n",
    "embedded = decoder_t.embedding(input_t)\n",
    "# appling dropout to embedded\n",
    "embedded_dropout = decoder_t.dropout(embedded)\n",
    "embedded_dropout.to(device)\n",
    "# passing through the lstm\n",
    "output, (hidden, cell) = decoder_t.lstm(embedded_dropout, (hidden_t, cell_t))\n",
    "# passing through the fully connected layer\n",
    "prediction_logit = decoder_t.fc_out(output)\n",
    "# appling softmax\n",
    "prediction = decoder_t.softmax(prediction_logit)\n",
    "\n",
    "print(f\"Input(target) tensor  [shape -> {input_t.shape[0]}]:-\\n\", input_t)\n",
    "print(f\"\\nEmbedded tokens  [shape -> {embedded.shape[0]}]:-\\n\", embedded)\n",
    "print(f\"\\nAfter dropout:-  [shape -> {embedded_dropout.shape[0]}]\\n\", embedded_dropout)\n",
    "#print(f\"\\nRNN Hidden:-     [shape -> {hidden.shape[0]}]\\n\", hidden)\n",
    "#print(f\"\\nRNN Cell:-       [shape -> {cell.shape[0]}]\\n\", cell)\n",
    "print(f\"\\nRNN Output:-     [shape -> {output.shape[0]}]\\n\", output)\n",
    "print(f\"\\nFC layer Out:-   [shape -> {prediction_logit.shape[0]}]\\n\", prediction_logit)\n",
    "print(f\"\\nSoftmax to Out:- [shape -> {prediction.shape[0]}]\\n\", prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db45b4d-453a-4e4f-95c7-b7cb5abb6707",
   "metadata": {},
   "source": [
    "#### Encoder-decoder connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37e541ec-6b69-45ac-ba22-9b49840d3872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder Input:- [shape - torch.Size([5, 1])]  (src_length, batch_size)\n",
      " tensor([[0],\n",
      "        [3],\n",
      "        [4],\n",
      "        [2],\n",
      "        [1]], device='cuda:0')\n",
      "\n",
      "Encoder Context vectors:-\n",
      "\n",
      "Hidden:- [shape - torch.Size([1, 1, 8])]  (1, batch_size, hid_dim)\n",
      " tensor([[[-0.1963,  0.0507, -0.1144,  0.0628, -0.0345, -0.0649, -0.2790,\n",
      "           0.0327]]], device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n",
      "\n",
      "Cell:- [shape - torch.Size([1, 1, 8])]  (1, batch_size, hid_dim)\n",
      " tensor([[[-0.2804,  0.1076, -0.1618,  0.2767, -0.1118, -0.1117, -0.4964,\n",
      "           0.0564]]], device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Target String:- [shape - torch.Size([5, 1])]  (trg_length, batch_size)\n",
      " tensor([[0],\n",
      "        [2],\n",
      "        [3],\n",
      "        [5],\n",
      "        [1]], device='cuda:0')\n",
      "\n",
      "First Decoder Input - <bos> [shape - torch.Size([1])]  (batch_size)\n",
      " tensor([0], device='cuda:0') \n",
      "\n",
      "Decoder output in each time step [shape -> torch.Size([5, 1, 6])]  (trg_length, batch_size, trg_vocab_size)\n",
      " tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-1.8682, -1.5401, -1.7110, -1.8023, -2.1168, -1.8005]],\n",
      "\n",
      "        [[-1.9152, -1.5334, -1.7368, -1.7468, -2.0958, -1.8109]],\n",
      "\n",
      "        [[-1.8655, -1.7337, -1.6260, -1.6185, -2.2626, -1.7746]],\n",
      "\n",
      "        [[-1.8555, -1.6796, -1.6189, -1.7607, -2.2350, -1.7140]]],\n",
      "       device='cuda:0', grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "# dummy data \n",
    "src = torch.tensor([[0,3,4,2,1]]) #where 0,3,4,2,1 are vocab indecies\n",
    "src = src.t().to(device)\n",
    "# instantiating the encoder model\n",
    "encoder_t = Encoder(vocab_len, emb_dim, hid_dim, n_layers, dropout_prob).to(device)\n",
    "# geting the encoder output\n",
    "hidden_t , cell_t = encoder_t(src)\n",
    "\n",
    "print(f\"Decoder Input:- [shape - {src.shape}]\",\" (src_length, batch_size)\\n\",src)\n",
    "print(\"\\nEncoder Context vectors:-\")\n",
    "print(f\"\\nHidden:- [shape - {hidden_t.shape}]\",\" (1, batch_size, hid_dim)\\n\",hidden_t)\n",
    "print(f\"\\nCell:- [shape - {cell_t.shape}]\",\" (1, batch_size, hid_dim)\\n\", cell_t)\n",
    "\n",
    "\n",
    "\n",
    "# dummy data \n",
    "trg = torch.tensor([[0],[2],[3],[5],[1]]).to(device) # 0 -> <bos>\n",
    "# instantiating the decoder model\n",
    "decoder_t = Decoder(output_dim, emb_dim, hid_dim, n_layers, dropout).to(device)\n",
    "\n",
    "# a tensor to store decoder opuputs at each time step\n",
    "batch_size = trg.shape[1]\n",
    "trg_len = trg.shape[0]\n",
    "trg_vocab_size = decoder_t.output_dim\n",
    "outputs_t = torch.zeros(trg_len, batch_size, trg_vocab_size).to(device)\n",
    "\n",
    "# the first input to the decoder is the <bos> token\n",
    "input = trg[0,:]\n",
    "\n",
    "print(\"\\n----------------------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(f\"\\nTarget String:- [shape - {trg.shape}]\",\" (trg_length, batch_size)\\n\",trg)\n",
    "print(f\"\\nFirst Decoder Input - <bos> [shape - {input.shape}]\",\" (batch_size)\\n\",input, \"\\n\")\n",
    "\n",
    "# looping through the trg length\n",
    "for t in range(1, trg_len):\n",
    "    output_t, hidden_t, cell_t = decoder_t(input, hidden_t, cell_t)\n",
    "    # storing the output in current time step\n",
    "    outputs_t[t] = output_t\n",
    "    #getting the predicted token  index\n",
    "    top_1 = output_t.argmax(1)\n",
    "    # deciding weather to use tracher forcing\n",
    "    teacher_force = random.random() < 0.5 # 0.5 -> teacher forcing ratio\n",
    "    input = trg[t] if teacher_force else top_1\n",
    "\n",
    "print(f\"Decoder output in each time step [shape -> {outputs_t.shape}]\",\" (trg_length, batch_size, trg_vocab_size)\\n\",outputs_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91a01920-d114-4eda-bc56-671b35b11a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [3],\n",
      "        [2]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# making predictions\n",
    "pred_tokens = outputs_t.argmax(2)\n",
    "print(pred_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0baee6-8f84-4a61-a2e0-8927e3b16d27",
   "metadata": {},
   "source": [
    "## Sequence-to-sequence model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "118bbb81-ff0f-4efd-b989-3fb6e07f7f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting encoder and decoder components to create the seq2seq model\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, trg_vocab):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.trg_vocab = trg_vocab\n",
    "\n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimension of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(device)\n",
    "\n",
    "        #last encoder hidden state\n",
    "        hidden, cell = self.encoder(src)\n",
    "        hidden.to(device)\n",
    "        cell.to(device)\n",
    "\n",
    "        #first input to decoder is <bos>\n",
    "        input = trg[0,:]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            output , hidden, cell = self.decoder(input, hidden, cell)\n",
    "             # storing the output from the current time step\n",
    "            outputs[t] = output\n",
    "            #getting the predicted token index\n",
    "            top_1 = output.argmax(1)\n",
    "            # deciding weather to use tracher forcing\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            input = trg[t] if  teacher_force else top_1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160a3d0c-0e31-4706-90c1-39dd469684d3",
   "metadata": {},
   "source": [
    "#### Example loss of one example document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2739a808-54e6-4aa6-a7a3-01cb9decd90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output in each time step [shape -> torch.Size([5, 1, 6])]  (trg_length, batch_size, trg_vocab_size)\n",
      " tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-1.8682, -1.5401, -1.7110, -1.8023, -2.1168, -1.8005]],\n",
      "\n",
      "        [[-1.9152, -1.5334, -1.7368, -1.7468, -2.0958, -1.8109]],\n",
      "\n",
      "        [[-1.8655, -1.7337, -1.6260, -1.6185, -2.2626, -1.7746]],\n",
      "\n",
      "        [[-1.8555, -1.6796, -1.6189, -1.7607, -2.2350, -1.7140]]],\n",
      "       device='cuda:0', grad_fn=<CopySlices>)\n",
      "\n",
      "Removing the 1st row <bos> and re-shaping the output [shape - torch.Size([4, 6])]  ([(trg_length-1) * batch_size], trg_vocab_size)\n",
      " tensor([[-1.8682, -1.5401, -1.7110, -1.8023, -2.1168, -1.8005],\n",
      "        [-1.9152, -1.5334, -1.7368, -1.7468, -2.0958, -1.8109],\n",
      "        [-1.8655, -1.7337, -1.6260, -1.6185, -2.2626, -1.7746],\n",
      "        [-1.8555, -1.6796, -1.6189, -1.7607, -2.2350, -1.7140]],\n",
      "       device='cuda:0')\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Target String:- [shape - torch.Size([5, 1])]  (trg_length, batch_size)\n",
      " tensor([[0],\n",
      "        [2],\n",
      "        [3],\n",
      "        [5],\n",
      "        [1]], device='cuda:0')\n",
      "\n",
      "Removing the 1st row <bos> and re-shaping the target [shape - torch.Size([4])]  [(trg_length-1) * batch_size]\n",
      " tensor([2, 3, 5, 1], device='cuda:0')\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Cross Entropy Loss:-  tensor(1.7280, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(f\"Decoder output in each time step [shape -> {outputs_t.shape}]\",\" (trg_length, batch_size, trg_vocab_size)\\n\",outputs_t)\n",
    "output_t1 = outputs_t.clone().detach()\n",
    "output_t1 = output_t1[1:].view(-1,output_dim)\n",
    "print(f\"\\nRemoving the 1st row <bos> and re-shaping the output [shape - {output_t1.shape}]\",\" ([(trg_length-1) * batch_size], trg_vocab_size)\\n\",output_t1)\n",
    "print(\"\\n-------------------------------------------------------------------------------------------------------------------------------------------\\n\")\n",
    "print(f\"\\nTarget String:- [shape - {trg.shape}]\",\" (trg_length, batch_size)\\n\",trg)\n",
    "trg1 = trg.clone().detach()\n",
    "trg1 = trg1[1:].contiguous().view(-1)\n",
    "print(f\"\\nRemoving the 1st row <bos> and re-shaping the target [shape - {trg1.shape}]\",\" [(trg_length-1) * batch_size]\\n\",trg1)\n",
    "print(\"\\n-------------------------------------------------------------------------------------------------------------------------------------------\\n\")\n",
    "print(f\"Cross Entropy Loss:- \", nn.CrossEntropyLoss()(output_t1,trg1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9c7158-252c-434e-b754-a0ff8a46de79",
   "metadata": {},
   "source": [
    "#### A function to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b59d9fd-27e0-48cb-ba52-1c3cc66414ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    train_iterator = tqdm(iterator, desc=\"Training\", leave=False)\n",
    "\n",
    "    for i, (src, trg) in enumerate(train_iterator):\n",
    "        # senting the src anf trg tensors to device\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "\n",
    "        # clearing the gradinet from previous batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # the models predictions - token probabilities\n",
    "        output = model(src, trg)\n",
    "\n",
    "        #----- trg shape -> [trg len, batch_size]\n",
    "        #----- output shape -> [trg_len, batch_size, output_dim]\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim) # removing <bos> and re-shaping\n",
    "        trg = trg[1:].contiguous().view(-1) # removing <bos> and re-shaping\n",
    "\n",
    "        #----- trg shape -> [(trg len -1) * batch_size]\n",
    "        #----- output shape -> [(trg len -1) * batch_size, output dim]\n",
    "\n",
    "        # computing the loss\n",
    "        loss = criterion(output, trg)\n",
    "        # compiting the gradient\n",
    "        loss.backward()\n",
    "        # cliping the gradien\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        # updating the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Updating the tqdm\n",
    "        train_iterator.set_postfix(loss=loss.item())\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(list(iterator))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f9a419-704c-4ac3-8519-7c097dcfad46",
   "metadata": {},
   "source": [
    "#### A function to evaluat the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b997dd32-fdf5-45f5-85fe-c64285dfb415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    valid_iterator = tqdm(iterator, desc=\"Training\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, (src, trg) in enumerate(valid_iterator):\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            output = model(src,trg,0) # 0 -> turning off teacher forcing\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim) # removing <bos> and re-shaping\n",
    "            trg = trg[1:].contiguous().view(-1) # removing <bos> and re-shaping\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            valid_iterator.set_postfix(loss=loss.item())\n",
    "            epoch_loss += loss.item()\n",
    "        return epoch_loss/len(list(iterator))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212b0da3-4620-4c3c-a445-37f19389111d",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c2a8ae1-3c01-4640-bde0-baf412617537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the code that has been created that contains all the transformation processes on data.\n",
    "%run Multi30K_de_en_dataloader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc64ed1b-ebbe-4a90-a71b-4443a7f70475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first batch in train_dataloader:- \n",
      "\n",
      "SRC tensor:-\n",
      " tensor([[    2,     2,     2,     2],\n",
      "        [    3,  5510,  5510, 12642],\n",
      "        [    1,     3,     3,     8],\n",
      "        [    1,     1,     1,  1701],\n",
      "        [    1,     1,     1,     3]], device='cuda:0')\n",
      "\n",
      "TRG tensor:-\n",
      " tensor([[   2,    2,    2,    2],\n",
      "        [   3, 6650,  216,    6],\n",
      "        [   1, 4623,  110, 3398],\n",
      "        [   1,  259, 3913,  202],\n",
      "        [   1,  172, 1650,  109],\n",
      "        [   1, 9953, 3823,   37],\n",
      "        [   1,  115,   71,    3],\n",
      "        [   1,  692, 2808,    1],\n",
      "        [   1, 3428, 2187,    1],\n",
      "        [   1,    5,    5,    1],\n",
      "        [   1,    3,    3,    1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# geting the train and valid data-loaders\n",
    "train_dataloader, valid_dataloader = get_translation_dataloaders(batch_size = 4)\n",
    "\n",
    "print(\"The first batch in train_dataloader:- \")\n",
    "src, trg = next(iter(train_dataloader))\n",
    "print(\"\\nSRC tensor:-\\n\", src)\n",
    "print(\"\\nTRG tensor:-\\n\", trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2ead1a7-a695-4b1e-b173-f7b3a4ac789a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 1000th batch in train_dataloader:- \n",
      "\n",
      "German tensor:-\n",
      " tensor([[   2,    2,    2,    2],\n",
      "        [  21,   84,    5,   14],\n",
      "        [ 773,   42,  315,   17],\n",
      "        [8314,  561,  149,  332],\n",
      "        [  10,   25,   22,   63],\n",
      "        [ 541,  458, 1121,    6],\n",
      "        [3125,   22,  104, 4505],\n",
      "        [ 174,   94,  901,  468],\n",
      "        [   4,    4,    4,    4],\n",
      "        [   3,    3,    3,    3]], device='cuda:0')\n",
      "\n",
      "English tensor:-\n",
      " tensor([[   2,    2,    2,    2],\n",
      "        [  19,   83, 2989,    6],\n",
      "        [  52,   17,   10,   16],\n",
      "        [ 266,  363,   56,  616],\n",
      "        [   7,  354,   18,    4],\n",
      "        [ 287,   20,   27,  477],\n",
      "        [1198, 1528,  515,   29],\n",
      "        [ 134,    5,   60,  175],\n",
      "        [  67,    3,  210,   28],\n",
      "        [   4,    1, 2688,  256],\n",
      "        [2266,    1,   63,  552],\n",
      "        [   5,    1,  811,    5],\n",
      "        [   3,    1,    5,    3],\n",
      "        [   1,    1,    3,    1]], device='cuda:0')\n",
      "\n",
      "German T tensor:-\n",
      " tensor([[   2,   21,  773, 8314,   10,  541, 3125,  174,    4,    3],\n",
      "        [   2,   84,   42,  561,   25,  458,   22,   94,    4,    3],\n",
      "        [   2,    5,  315,  149,   22, 1121,  104,  901,    4,    3],\n",
      "        [   2,   14,   17,  332,   63,    6, 4505,  468,    4,    3]],\n",
      "       device='cuda:0')\n",
      "\n",
      "English T tensor:-\n",
      " tensor([[   2,   19,   52,  266,    7,  287, 1198,  134,   67,    4, 2266,    5,\n",
      "            3,    1],\n",
      "        [   2,   83,   17,  363,  354,   20, 1528,    5,    3,    1,    1,    1,\n",
      "            1,    1],\n",
      "        [   2, 2989,   10,   56,   18,   27,  515,   60,  210, 2688,   63,  811,\n",
      "            5,    3],\n",
      "        [   2,    6,   16,  616,    4,  477,   29,  175,   28,  256,  552,    5,\n",
      "            3,    1]], device='cuda:0')\n",
      "\n",
      "German Text:-\n",
      "\n",
      "<bos> wearing passing diapered is drums decorating across a <eos>\n",
      "<bos> near walking arts young view people stands a <eos>\n",
      "<bos> . swing it people tractor stand carnival a <eos>\n",
      "<bos> with are going through A railway attire a <eos>\n",
      "\n",
      "English Text:-\n",
      "\n",
      "<bos> Two women workers in hats serving food from a stall . <eos> <pad>\n",
      "<bos> People are gathered outdoors at nighttime . <eos> <pad> <pad> <pad> <pad> <pad>\n",
      "<bos> Bicyclist is looking to his right as he travels through traffic . <eos>\n",
      "<bos> A woman drinks a beer while watching an outdoor concert . <eos> <pad>\n"
     ]
    }
   ],
   "source": [
    "# printing the data from an example batch\n",
    "data_itr = iter(train_dataloader)\n",
    "for n in range(1000):\n",
    "    german, english = next(data_itr)\n",
    "\n",
    "print(\"The 1000th batch in train_dataloader:- \")\n",
    "print(\"\\nGerman tensor:-\\n\", german)\n",
    "print(\"\\nEnglish tensor:-\\n\", english)\n",
    "\n",
    "german = german.T\n",
    "english = english.T\n",
    "\n",
    "print(\"\\nGerman T tensor:-\\n\", german)\n",
    "print(\"\\nEnglish T tensor:-\\n\", english)\n",
    "\n",
    "# printing the actual text\n",
    "print(\"\\nGerman Text:-\\n\") # The german text is in reverse\n",
    "for e in german:\n",
    "    print(index_to_eng(e))\n",
    "\n",
    "print(\"\\nEnglish Text:-\\n\")\n",
    "for e in english:\n",
    "    print(index_to_eng(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b11619c-eeb6-4332-a473-1fb5cd63af04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2,   19,   52,  266,    7,  287, 1198,  134,   67,    4, 2266,    5,\n",
       "            3,    1],\n",
       "        [   2,   83,   17,  363,  354,   20, 1528,    5,    3,    1,    1,    1,\n",
       "            1,    1],\n",
       "        [   2, 2989,   10,   56,   18,   27,  515,   60,  210, 2688,   63,  811,\n",
       "            5,    3],\n",
       "        [   2,    6,   16,  616,    4,  477,   29,  175,   28,  256,  552,    5,\n",
       "            3,    1]], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec24355e-492f-4487-a806-d969e7882e8e",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258d3fdc-1059-48ee-89d5-7cb2176ace50",
   "metadata": {},
   "source": [
    "#### Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27c93f63-af1c-4a71-87bb-768ec663bb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234 # for result reproducibility\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4f62460-a211-4711-b099-7eb4307b250f",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(vocab_transform['de'])\n",
    "OUTPUT_DIM = len(vocab_transform['en'])\n",
    "ENC_EMB_DIM = 128\n",
    "DEC_EMB_DIM = 128\n",
    "HID_DIM = 256\n",
    "N_LAYERS = 1\n",
    "ENC_DROPOUT = 0.3\n",
    "DEC_DROPOUT = 0.3\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec, trg_vocab=vocab_transform['en']).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d98559dc-3822-4d86-94dd-f4463686ab75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(19214, 128)\n",
       "    (lstm): LSTM(128, 256, dropout=0.3)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(10837, 128)\n",
       "    (lstm): LSTM(128, 256, dropout=0.3)\n",
       "    (fc_out): Linear(in_features=256, out_features=10837, bias=True)\n",
       "    (softmax): LogSoftmax(dim=1)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (trg_vocab): Vocab()\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initializing the initial weights\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93cf2e54-0e86-4b93-a0fd-fa12e2f83e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 7422165:, trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# no.of trainable paraments \n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"The model has {count_parameters(model)}:, trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dacf1c9c-2f9c-4e08-a8aa-72c96658d2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the loss criterion, optimizer using functions from the PyTorch Library \n",
    "optimizer = optim.Adam(model.parameters())\n",
    "PAD_IDX = vocab_transform['en'].get_stoi()['<pad>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "772f76fa-e2ef-467a-9d39-8e8094d43c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a helper function for claculating time taken for training\n",
    "def epoch_time(start_time, end_time):\n",
    "    elasped_time = end_time - start_time\n",
    "    elasped_mins = int(elasped_time/60)\n",
    "    elasped_secs = int(elasped_time - (elasped_mins * 60))\n",
    "    return elasped_mins, elasped_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "09c089d1-4f40-4644-8659-7713f69a9e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 3m 50s\n",
      "\tTrain Loss: 4.384 | Train PPL:  80.174\n",
      "\t Val. Loss: 5.185 | Train PPL: 178.531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Time: 4m 3s\n",
      "\tTrain Loss: 3.673 | Train PPL:  39.356\n",
      "\t Val. Loss: 4.781 | Train PPL: 119.235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Time: 4m 2s\n",
      "\tTrain Loss: 3.268 | Train PPL:  26.271\n",
      "\t Val. Loss: 4.428 | Train PPL:  83.760\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "N_EPOCHS = 3\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "best_train_loss = float('inf')\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "train_PPLs = []\n",
    "valid_PPLs = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_dataloader, optimizer, criterion, CLIP)\n",
    "    train_ppl = math.exp(train_loss)\n",
    "    valid_loss = evaluate(model, valid_dataloader, criterion)\n",
    "    valid_ppl = math.exp(valid_loss)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"RNN-TR-model.pt\")\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    train_PPLs.append(train_ppl)\n",
    "    valid_losses.append(valid_loss)\n",
    "    valid_PPLs.append(valid_ppl)\n",
    "\n",
    "    print(f\"Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:.3f} | Train PPL: {train_ppl:7.3f}\")\n",
    "    print(f\"\\t Val. Loss: {valid_loss:.3f} | Train PPL: {valid_ppl:7.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3df65f-b7ca-40d5-8a7c-4326d5ca1080",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Visualizing the training and validation loss\n",
    "\n",
    "# Create a list of epoch numbers\n",
    "epochs = [epoch+1 for epoch in range(N_EPOCHS)]\n",
    "\n",
    "# Create the figure and axes\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plotting the training and validation loss\n",
    "ax1.plot(epochs, train_losses, label='Train Loss', color='blue')\n",
    "ax1.plot(epochs, valid_losses, label='Validation Loss', color='orange')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss/PPL')\n",
    "\n",
    "# Plotting the training and validation perplexity\n",
    "ax2.plot(epochs, train_PPLs, label='Train PPL', color='green')\n",
    "ax2.plot(epochs, valid_PPLs, label='Validation PPL', color='red')\n",
    "ax2.set_ylabel('Perplexity')\n",
    "\n",
    "# Adjust the y-axis scaling for PPL plot\n",
    "ax2.set_ylim(bottom=min(min(train_PPLs), min(valid_PPLs)) - 10, top=max(max(train_PPLs), max(valid_PPLs)) + 10)\n",
    "\n",
    "# Set the legend\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "lines = lines1 + lines2\n",
    "labels = labels1 + labels2\n",
    "ax1.legend(lines, labels, loc='upper right')\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b0d8f0-0a73-4360-9b40-d9e196a36f3f",
   "metadata": {},
   "source": [
    "## Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c61e6c-7564-4dcb-88c3-dbdeb6a23496",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
